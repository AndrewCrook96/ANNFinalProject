{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8018ee22-d53f-4ed1-9b32-a631485abfa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-06 15:47:47.711274: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-06 15:47:47.728165: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-06 15:47:47.733136: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-06 15:47:47.746559: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import logging\n",
    "import os\n",
    "\n",
    "def load_and_preprocess_data(hdf5_file, file_idx, spatial_idx, time_start_idx, time_end_idx):\n",
    "    \"\"\"\n",
    "    Load and preprocess data from the HDF5 file.\n",
    "    \n",
    "    Parameters:\n",
    "        hdf5_file (str): Path to the HDF5 file.\n",
    "        file_idx (int): Index of the file.\n",
    "        spatial_idx (int): Index of the spatial chunk to load.\n",
    "        time_start_idx (int): Start index for time chunks.\n",
    "        time_end_idx (int): End index for time chunks (inclusive).\n",
    "    Returns:\n",
    "        raw_data (np.array): Raw data (5000x100).\n",
    "        fft_data (np.array): FFT data (5000x100) (magnitude spectrum, normalized).\n",
    "    \"\"\"\n",
    "    with h5py.File(hdf5_file, 'r') as f:\n",
    "        raw_data = []\n",
    "        for t in range(time_start_idx, time_end_idx + 1):\n",
    "            chunk_name = f'chunk_{file_idx}_{t}_{spatial_idx}'\n",
    "            chunk = f[chunk_name][:]\n",
    "            raw_data.append(chunk)\n",
    "    \n",
    "    raw_data = np.concatenate(raw_data, axis=0)  # Combine time chunks\n",
    "    \n",
    "    # Perform FFT on the concatenated time chunk\n",
    "    fft_data = np.abs(np.fft.fft(raw_data, axis=0))  # FFT along time axis\n",
    "    \n",
    "    # Normalize raw and FFT data\n",
    "    raw_mean, raw_std = np.mean(raw_data), np.std(raw_data)\n",
    "    raw_data = (raw_data - raw_mean) / (raw_std + 1e-6)\n",
    "    \n",
    "    fft_mean, fft_std = np.mean(fft_data), np.std(fft_data)\n",
    "    fft_data = (fft_data - fft_mean) / (fft_std + 1e-6)\n",
    "    \n",
    "    return raw_data, fft_data\n",
    "\n",
    "def detect_anomalies(hdf5_file, model_path, output_dir='anomaly_results'):\n",
    "    \"\"\"\n",
    "    Detect anomalies in DAS data using autoencoder reconstruction error.\n",
    "    \n",
    "    Parameters:\n",
    "        hdf5_file (str): Path to the HDF5 file.\n",
    "        model_path (str): Path to the trained autoencoder model.\n",
    "        output_dir (str): Directory to save anomaly results.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set up logging\n",
    "    logging.basicConfig(level=logging.INFO, format='%(message)s')\n",
    "    \n",
    "    # Load trained autoencoder\n",
    "    autoencoder = keras.models.load_model(model_path)\n",
    "    \n",
    "    # Create output directory\n",
    "    import os\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Prepare to track anomalies\n",
    "    anomalies = []\n",
    "    \n",
    "    # Iterate through data sections\n",
    "    with h5py.File(hdf5_file, 'r') as raw_f:\n",
    "        num_files = len(raw_f.keys()) // (87 * 30) - 9\n",
    "        for file_idx in range(num_files):\n",
    "            for spatial_idx in range(87):\n",
    "                print(spatial_idx)\n",
    "                for time_idx in range(0, 27, 3):\n",
    "                    # Load and preprocess data\n",
    "                    raw_data, fft_data = load_and_preprocess_data(\n",
    "                        hdf5_file, file_idx, spatial_idx, time_idx, time_idx + 4\n",
    "                    )\n",
    "                    \n",
    "                    # Prepare data for reconstruction\n",
    "                    raw_input = raw_data[np.newaxis, ...]\n",
    "                    fft_input = fft_data[np.newaxis, ...]\n",
    "                    \n",
    "                    # Reconstruct signals\n",
    "                    raw_recon = autoencoder.predict(raw_input, verbose=0)[0]\n",
    "                    fft_recon = autoencoder.predict(fft_input, verbose=0)[0]\n",
    "                    \n",
    "                    # Calculate MSE for time and frequency domains\n",
    "                    raw_mse = np.mean((raw_data - raw_recon)**2)\n",
    "                    fft_mse = np.mean((fft_data - fft_recon)**2)\n",
    "                    \n",
    "                    anomalies.append({\n",
    "                        'file_idx': file_idx,\n",
    "                        'spatial_idx': spatial_idx,\n",
    "                        'time_idx': time_idx,\n",
    "                        'raw_mse': raw_mse,\n",
    "                        'fft_mse': fft_mse,\n",
    "                        'raw_data': raw_data,\n",
    "                        'fft_data': fft_data,\n",
    "                        'raw_recon': raw_recon,\n",
    "                        'fft_recon': fft_recon\n",
    "                    })\n",
    "\n",
    "    # Log the final message\n",
    "    logging.info(\"Anomaly detection complete. Results saved in %s\", output_dir)\n",
    "    \n",
    "    # Calculate anomaly scores using standard deviation\n",
    "    raw_mses = [a['raw_mse'] for a in anomalies]\n",
    "    fft_mses = [a['fft_mse'] for a in anomalies]\n",
    "    \n",
    "    raw_z_scores = np.abs(stats.zscore(raw_mses))\n",
    "    fft_z_scores = np.abs(stats.zscore(fft_mses))\n",
    "    \n",
    "    # Combine Z-scores (you can adjust weighting if needed)\n",
    "    combined_z_scores = (raw_z_scores + fft_z_scores) / 2\n",
    "    \n",
    "    for a, z_score in zip(anomalies, combined_z_scores):\n",
    "        a['z_score'] = z_score\n",
    "    \n",
    "    # Sort anomalies by z-score\n",
    "    sorted_anomalies = sorted(anomalies, key=lambda x: x['z_score'], reverse=True)\n",
    "    \n",
    "    '''# Visualize top 5 and bottom 5 anomalies\n",
    "    def plot_anomaly(anomaly, idx, is_top=True):\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # Time domain plot\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.title(f'{\"Top\" if is_top else \"Bottom\"} Anomaly {idx+1} - Time Domain\\n'\n",
    "                  f'File: {anomaly[\"file_idx\"]}, Spatial: {anomaly[\"spatial_idx\"]}, '\n",
    "                  f'Z-Score: {anomaly[\"z_score\"]:.2f}')\n",
    "        plt.plot(anomaly['raw_data'], label='Original')\n",
    "        plt.plot(anomaly['raw_recon'], label='Reconstructed', linestyle='--')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Frequency domain plot\n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.title(f'{\"Top\" if is_top else \"Bottom\"} Anomaly {idx+1} - Frequency Domain')\n",
    "        plt.plot(anomaly['fft_data'], label='Original FFT')\n",
    "        plt.plot(anomaly['fft_recon'], label='Reconstructed FFT', linestyle='--')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, \n",
    "                    f'{\"top\" if is_top else \"bottom\"}_anomaly_{idx+1}.png'))\n",
    "        plt.close()\n",
    "    \n",
    "    # Plot top 5 and bottom 5 anomalies\n",
    "    for i in range(5):\n",
    "        plot_anomaly(sorted_anomalies[i], i, is_top=True)\n",
    "        plot_anomaly(sorted_anomalies[-(i+1)], i, is_top=False)\n",
    "    \n",
    "    # Save anomaly details to CSV\n",
    "    import pandas as pd\n",
    "    anomaly_df = pd.DataFrame(sorted_anomalies)\n",
    "    anomaly_df.to_csv(os.path.join(output_dir, 'anomaly_details.csv'), index=False)\n",
    "    \n",
    "    print(\"Anomaly detection complete. Results saved in\", output_dir)'''\n",
    "\n",
    "\n",
    "    import os\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pandas as pd\n",
    "   # import numpy as np\n",
    "    \n",
    "    # Define function to plot an anomaly\n",
    "    def plot_anomaly(anomaly, idx):\n",
    "        plt.figure(figsize=(15, 10))\n",
    "    \n",
    "        # Time domain plot \n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.title(f'Anomaly {idx+1} - Time Domain\\n'\n",
    "                  f'File: {anomaly[\"file_idx\"]}, Spatial: {anomaly[\"spatial_idx\"]}, '\n",
    "                  f'Z-Score: {anomaly[\"z_score\"]:.2f}')\n",
    "    \n",
    "        # Select 10 evenly spaced points for plotting\n",
    "        raw_indices = np.linspace(0, 10,len(anomaly['raw_data']) - 1, dtype=int)\n",
    "        recon_indices = np.linspace(0, 10, len(anomaly['raw_recon']) - 1, dtype=int)\n",
    "    \n",
    "        # Plot raw and reconstructed data\n",
    "        plt.plot(raw_indices, [anomaly['raw_data'][i] for i in raw_indices], label='Raw Data', color='blue')\n",
    "        plt.plot(recon_indices, [anomaly['raw_recon'][i] for i in recon_indices], label='Reconstructed Data', linestyle='--', color='orange')\n",
    "        #plt.legend()\n",
    "    \n",
    "        # Frequency domain plot\n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.title(f'Anomaly {idx+1} - Frequency Domain')\n",
    "    \n",
    "        # Select 10 evenly spaced points for FFT data\n",
    "        fft_indices = np.linspace(0, len(anomaly['fft_data']) - 1, 10, dtype=int)\n",
    "        fft_recon_indices = np.linspace(0, len(anomaly['fft_recon']) - 1, 10, dtype=int)\n",
    "    \n",
    "        # Plot FFT raw and reconstructed data\n",
    "        plt.plot(fft_indices, [anomaly['fft_data'][i] for i in fft_indices], label='Raw FFT Data', color='blue')\n",
    "        plt.plot(fft_recon_indices, [anomaly['fft_recon'][i] for i in fft_recon_indices], label='Reconstructed FFT Data', linestyle='--', color='orange')\n",
    "        #plt.legend()\n",
    "    \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, f'anomaly_{idx+1}.png'))\n",
    "        plt.close()\n",
    "    \n",
    "    # Filter anomalies with Z-score > 3.1\n",
    "    filtered_anomalies = [anomaly for anomaly in sorted_anomalies if anomaly[\"z_score\"] > 3.1]\n",
    "    \n",
    "    # Plot each filtered anomaly\n",
    "    for idx, anomaly in enumerate(filtered_anomalies):\n",
    "        plot_anomaly(anomaly, idx)\n",
    "    \n",
    "    # Save anomaly details to CSV\n",
    "    if filtered_anomalies:\n",
    "        anomaly_df = pd.DataFrame(filtered_anomalies)\n",
    "        anomaly_df.to_csv(os.path.join(output_dir, 'filtered_anomaly_details.csv'), index=False)\n",
    "        print(f\"Anomaly detection complete. {len(filtered_anomalies)} results saved in {output_dir}\")\n",
    "    else:\n",
    "        print(\"No anomalies with Z-score > 3 were found.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75374693-1da8-4eca-91d6-486daeaa247b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1733525271.182097    6079 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1733525271.222670    6079 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1733525271.222772    6079 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1733525271.229474    6079 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1733525271.229682    6079 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1733525271.229785    6079 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1733525271.456147    6079 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1733525271.456341    6079 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-12-06 15:47:51.456359: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2112] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1733525271.456485    6079 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-12-06 15:47:51.456524: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4057 MB memory:  -> device: 0, name: Quadro RTX 3000, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2024-12-06 15:47:56.741077: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 256000000 exceeds 10% of free system memory.\n",
      "/home/acrook/anaconda3/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:576: UserWarning: Skipping variable loading for optimizer 'adam', because it has 30 variables whereas the saved optimizer has 34 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1733525277.394393    6140 service.cc:146] XLA service 0x7f598c003d10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1733525277.394449    6140 service.cc:154]   StreamExecutor device (0): Quadro RTX 3000, Compute Capability 7.5\n",
      "2024-12-06 15:47:57.416304: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-12-06 15:47:57.475222: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 90101\n",
      "I0000 00:00:1733525278.980989    6140 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Anomaly detection complete. Results saved in anomaly_results\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly detection complete. 4 results saved in anomaly_results\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Paths to update\n",
    "hdf5_file_path = 'raw_data.h5'\n",
    "model_path = 'autoencoder2.keras'\n",
    "\n",
    "detect_anomalies(hdf5_file_path, model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc47456d-4fc0-4a81-b613-75730dd9fcb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
